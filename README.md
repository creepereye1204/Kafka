### 개요
이 문서는 서비스 전반에 Kafka를 도입한 배경과 의사결정 이유, 기존 선택지(RabbitMQ/gRPC)와의 비교, 그리고 운영 시 권장 설정을 정리한다. 목표는 데이터 정합성과 장애 내성 강화, 재처리 가능성 확보, 서비스 간 결합도 감소다.

### 도입 배경
- textneck hub의 데이터 정합성 보장과 장애 내성 확보가 점점 더 중요해짐
- DB CRUD 실패나 다운스트림 장애 시에도 이벤트 유실 없이 복구·재처리 필요
- 서비스 간 결합도를 낮추고, 독립 배포/스케일링이 가능한 아키텍처 지향
- 장기 보관 이벤트 로그 기반의 분석·재처리·감사 요구 증가

### 왜 Kafka인가
- 강한 내구성 및 재처리 용이성
  - Append-only 로그와 보관 정책(retention)으로 과거 이벤트를 재구독/재처리 가능
  - 프로듀서 멱등성(enable.idempotence)과 트랜잭션으로 중복/정합성 이슈 최소화
- 순서 보장과 확장성
  - 파티션 키 기준 순서 보장, 파티션 수 확장으로 고처리량 대응이 쉬움
- 복구 전략 단순화
  - 컨슈머 그룹 오프셋만으로 실패 구간 재처리 가능(롤백/리플레이 전략 단순)
- 생태계/운영 도구 풍부
  - Connect, Streams, ksqlDB, 모니터링 스택 등 확장·운영 편의성이 높음

### gRPC 대신 MOM(Kafka)을 선택한 이유
- 결합도 감소
  - gRPC는 동기 RPC 특성상 호출자-피호출자 간 가용성·타이밍 결합이 큼
  - Kafka는 비동기 메시징으로 시간/가용성 분리, 느슨한 결합 실현
- 유연한 장애 대응
  - 다운스트림이 일시 장애여도 메시지는 로그에 안전히 적재, 복구 후 재처리
- 서비스 응집도 유지
  - 각 서비스 내부 도메인 로직은 자체 응집도를 유지하고, 서비스 간 통신은 이벤트로 느슨하게 연결


### RabbitMQ 관련 정정
- “RabbitMQ는 저장 기능이 없어서 서버가 꺼지면 발행물을 까먹는다”는 표현은 사실과 다름.
  - RabbitMQ도 영속 저장을 지원하며, Durable Queue + Persistent Message + Publisher Confirms/Quorum Queues로 메시지 유실 없이 운용 가능.
  - 다만 RabbitMQ는 장기 보관/재처리(이벤트 로그로서의 리플레이)보다는 “전달”에 초점이 맞춰져 있고, 대규모 장기 리플레이·시간여행 처리가 필요할 때는 Kafka가 구조적으로 유리.


### 운영/설정 제안
- 프로듀서
  - acks=all, enable.idempotence=true
  - linger.ms=5~20ms, batch.size=128~512KB, compression=zstd 또는 lz4
  - 메시지 키를 도메인 키(예: user_id)로 고정해 파티션 내 순서 보장
- 컨슈머
  - 자동 커밋은 신중히
  - 재시도·D/L/Q(Dead Letter Topic) 전략 명확화
- 스키마 관리
  - 스키마 레지스트리(Avro/Protobuf/JSON-Schema)로 진화 전략 정의
  - 헤더로 스키마 버전 명시, 호환성 정책(Backward/Full) 채택
- 모니터링
  - Lag 모니터링, 프로듀서·컨슈머 에러율, 브로커 헬스 체크 대시보드 구성

### RabbitMQ → Kafka 전환 메모
- Outbox 패턴으로 DB 트랜잭션과 이벤트 발행 일관성 확보
- 이중 송출 기간 운영(그레이스풀 마이그레이션) + 컨슈머 이중화로 리스크 완화
- 이벤트 스키마 버저닝 및 컨슈머 호환성 테스트 필수


